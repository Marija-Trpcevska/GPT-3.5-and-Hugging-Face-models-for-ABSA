C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\Scripts\python.exe C:\Users\Marija\Desktop\ABSADatasets-2.0\train_apc.py 
No CUDA GPU found in your device
Version 0.9.6 of metric_visualizer is outdated. Version 0.9.7 was released 1 day ago.
No CUDA GPU found in your device
Version 0.9.6 of metric_visualizer is outdated. Version 0.9.7 was released 1 day ago.
[2023-04-14 16:36:10] (2.2.1) PyABSA(2.2.1): 
[New Feature] Aspect Sentiment Triplet Extraction from v2.1.0 test version (https://github.com/yangheng95/PyABSA/tree/v2/examples-v2/aspect_sentiment_triplet_extration)
[New Feature] Aspect CategoryOpinion Sentiment Quadruple Extraction from v2.2.0 test version (https://github.com/yangheng95/PyABSA/tree/v2/examples-v2/aspect_opinion_sentiment_category_extraction)

If you find any problems, please report them on GitHub. Thanks!
The v2.x versions are not compatible with Google Colab. Please downgrade to 1.16.27.

[2023-04-14 16:36:10] (2.2.1) Set Model Device: cpu
[2023-04-14 16:36:10] (2.2.1) Device Name: Unknown
2023-04-14 16:36:10,347 INFO: PyABSA version: 2.2.1
2023-04-14 16:36:10,348 INFO: Transformers version: 4.27.4
2023-04-14 16:36:10,348 INFO: Torch version: 2.0.0+cpu+cudaNone
2023-04-14 16:36:10,348 INFO: Device: Unknown
[2023-04-14 16:36:10] (2.2.1) Loading dataset from local path: .\integrated_datasets\apc_datasets\100.CustomDataset
[2023-04-14 16:36:10] (2.2.1) Make sure the dataset file names are in the correct format, e.g., train.txt, test.txt, valid.txt
[2023-04-14 16:36:10] (2.2.1) Try to load ['.\\integrated_datasets\\apc_datasets\\100.CustomDataset'] dataset from local disk
C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\lib\site-packages\transformers\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of the model checkpoint at yangheng/deberta-v3-base-absa-v1.1 were not used when initializing DebertaV2Model: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2023-04-14 16:36:15] (2.2.1) PyABSA(2.2.1): 
[New Feature] Aspect Sentiment Triplet Extraction from v2.1.0 test version (https://github.com/yangheng95/PyABSA/tree/v2/examples-v2/aspect_sentiment_triplet_extration)
[New Feature] Aspect CategoryOpinion Sentiment Quadruple Extraction from v2.2.0 test version (https://github.com/yangheng95/PyABSA/tree/v2/examples-v2/aspect_opinion_sentiment_category_extraction)

If you find any problems, please report them on GitHub. Thanks!
The v2.x versions are not compatible with Google Colab. Please downgrade to 1.16.27.

[2023-04-14 16:36:15] (2.2.1) Set Model Device: cpu
[2023-04-14 16:36:15] (2.2.1) Device Name: Unknown
C:\Program Files\Python310\lib\multiprocessing\pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>
  _warn(f"unclosed running multiprocessing pool {self!r}",
ResourceWarning: Enable tracemalloc to get the object allocation traceback
2023-04-14 16:36:15,998 INFO: PyABSA version: 2.2.1
2023-04-14 16:36:15,998 INFO: Transformers version: 4.27.4
2023-04-14 16:36:15,998 INFO: Torch version: 2.0.0+cpu+cudaNone
2023-04-14 16:36:15,998 INFO: Device: Unknown
[2023-04-14 16:36:16] (2.2.1) Loading dataset from local path: .\integrated_datasets\apc_datasets\100.CustomDataset
[2023-04-14 16:36:16] (2.2.1) Make sure the dataset file names are in the correct format, e.g., train.txt, test.txt, valid.txt
[2023-04-14 16:36:16] (2.2.1) Try to load ['.\\integrated_datasets\\apc_datasets\\100.CustomDataset'] dataset from local disk
C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\lib\site-packages\transformers\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Some weights of the model checkpoint at yangheng/deberta-v3-base-absa-v1.1 were not used when initializing DebertaV2Model: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2023-04-14 16:36:20,907 INFO: Load dataset from .\integrated_datasets\apc_datasets\100.CustomDataset\custom.train.txt
preparing dataloader: 100%|██████████| 380/380 [00:00<00:00, 2187.90it/s]
2023-04-14 16:36:21,101 INFO: Dataset Label Details: {'Positive': 293, 'Negative': 40, 'Neutral': 44, 'Sum': 377}
2023-04-14 16:36:21,257 INFO: train data examples:
 [{'ex_id': tensor(0), 'text_raw': 'They will be serving you filter coffee unfortunately', 'text_spc': '[CLS] They will be serving you filter coffee unfortunately [SEP] filter coffee [SEP]', 'aspect': 'filter coffee', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([   1,  450,  296,  282, 2511,  274, 3845, 1818, 6375,    2, 3845, 1818,
           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(1), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100,    1,    1, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([   1,  450,  296,  282, 2511,  274, 3845, 1818, 6375,    2, 3845, 1818,
           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([   1,  450,  296,  282, 2511,  274, 3845, 1818, 6375,    2, 3845, 1818,
           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0]), 'right_dist': tensor(0)}, {'ex_id': tensor(1), 'text_raw': 'You have to try the carrot juice .', 'text_spc': '[CLS] You have to try the carrot juice . [SEP] carrot juice [SEP]', 'aspect': 'carrot juice', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1,   367,   286,   264,   687,   262, 18605,  4762,   323,     2,
        18605,  4762,     2,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100,    2,    2, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1,   367,   286,   264,   687,   262, 18605,  4762,   323,     2,
        18605,  4762,     2,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.7000, 0.8000, 0.9000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1,   367,   286,   264,   687,   262, 18605,  4762,   323,     2,
        18605,  4762,     2,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'right_dist': tensor(0)}]
2023-04-14 16:36:21,639 INFO: Load dataset from .\integrated_datasets\apc_datasets\100.CustomDataset\custom.test.txt
preparing dataloader: 100%|██████████| 87/87 [00:00<00:00, 2276.13it/s]
2023-04-14 16:36:21,686 INFO: Dataset Label Details: {'Negative': 9, 'Neutral': 6, 'Positive': 72, 'Sum': 87}
2023-04-14 16:36:21,721 INFO: test data examples:
 [{'ex_id': tensor(0), 'text_raw': "Beat authentic taco truck on the Westside. You can't go wrong with their al pastor tacos. Top with their homemade cucumber/onion/jalapeño mixture for a fresh kick!", 'text_spc': "[CLS] Beat authentic taco truck on the Westside. You can't go wrong with their al pastor tacos. Top with their homemade cucumber/onion/jalapeño mixture for a fresh kick! [SEP] taco truck [SEP]", 'aspect': 'taco truck', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9744,
        0.9487, 0.9231, 0.8974, 0.8718, 0.8462, 0.8205, 0.7949, 0.7692, 0.7436,
        0.7179, 0.6923, 0.6667, 0.6410, 0.6154, 0.5897, 0.5641, 0.5385, 0.5128,
        0.4872, 0.4615, 0.4359, 0.4103, 0.3846, 0.3590, 0.3333, 0.3077, 0.2821,
        0.2564, 0.2308, 0.2051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9744,
        0.9487, 0.9231, 0.8974, 0.8718, 0.8462, 0.8205, 0.7949, 0.7692, 0.7436,
        0.7179, 0.6923, 0.6667, 0.6410, 0.6154, 0.5897, 0.5641, 0.5385, 0.5128,
        0.4872, 0.4615, 0.4359, 0.4103, 0.3846, 0.3590, 0.3333, 0.3077, 0.2821,
        0.2564, 0.2308, 0.2051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1, 12614,  5529, 27164,  2972,   277,   262, 54087,   260,   367,
          295,   280,   297,   424,  1299,   275,   308,  1564,  9686, 23664,
          260,  2482,   275,   308,  8267, 20385,   320, 90931,   320, 81976,
         1492, 73251,  4025,   270,   266,  1576,  4144,   300,     2, 27164,
         2972,     2,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100,    2,    2, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9744,
        0.9487, 0.9231, 0.8974, 0.8718, 0.8462, 0.8205, 0.7949, 0.7692, 0.7436,
        0.7179, 0.6923, 0.6667, 0.6410, 0.6154, 0.5897, 0.5641, 0.5385, 0.5128,
        0.4872, 0.4615, 0.4359, 0.4103, 0.3846, 0.3590, 0.3333, 0.3077, 0.2821,
        0.2564, 0.2308, 0.2051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1, 12614,  5529, 27164,  2972,   277,   262, 54087,   260,   367,
          295,   280,   297,   424,  1299,   275,   308,  1564,  9686, 23664,
          260,  2482,   275,   308,  8267, 20385,   320, 90931,   320, 81976,
         1492, 73251,  4025,   270,   266,  1576,  4144,   300,     2, 27164,
         2972,     2,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9744,
        0.9487, 0.9231, 0.8974, 0.8718, 0.8462, 0.8205, 0.7949, 0.7692, 0.7436,
        0.7179, 0.6923, 0.6667, 0.6410, 0.6154, 0.5897, 0.5641, 0.5385, 0.5128,
        0.4872, 0.4615, 0.4359, 0.4103, 0.3846, 0.3590, 0.3333, 0.3077, 0.2821,
        0.2564, 0.2308, 0.2051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1, 12614,  5529, 27164,  2972,   277,   262, 54087,   260,   367,
          295,   280,   297,   424,  1299,   275,   308,  1564,  9686, 23664,
          260,  2482,   275,   308,  8267, 20385,   320, 90931,   320, 81976,
         1492, 73251,  4025,   270,   266,  1576,  4144,   300,     2, 27164,
         2972,     2,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'right_dist': tensor(0)}, {'ex_id': tensor(1), 'text_raw': "Don't forget to order creme brulee as a dessert", 'text_spc': "[CLS] Don't forget to order creme brulee as a dessert [SEP] creme brulee [SEP]", 'aspect': 'creme brulee', 'aspect_position': tensor(0, dtype=torch.int32), 'lca_ids': tensor([0.6923, 0.7692, 0.8462, 0.9231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.6923, 0.7692, 0.8462, 0.9231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([     1,   1310,    280,    297,   2308,    264,    556,  40390, 115258,
           283,    266,   8308,      2,  40390, 115258,      2,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100,    2,    2, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0, dtype=torch.int32), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.6923, 0.7692, 0.8462, 0.9231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([     1,   1310,    280,    297,   2308,    264,    556,  40390, 115258,
           283,    266,   8308,      2,  40390, 115258,      2,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.6923, 0.7692, 0.8462, 0.9231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([     1,   1310,    280,    297,   2308,    264,    556,  40390, 115258,
           283,    266,   8308,      2,  40390, 115258,      2,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0,      0,
             0,      0,      0,      0,      0,      0,      0,      0]), 'right_dist': tensor(0)}]
2023-04-14 16:36:21,722 INFO: valid data examples:
 []
[2023-04-14 16:36:21] (2.2.1) Caching dataset... please remove cached dataset if any problem happens.
2023-04-14 16:36:22,450 INFO: Model Architecture:
 APCEnsembler(
  (models): ModuleList(
    (0): FAST_LSA_T_V2(
      (bert4global): DebertaV2Model(
        (embeddings): DebertaV2Embeddings(
          (word_embeddings): Embedding(128100, 768, padding_idx=0)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
          (dropout): StableDropout()
        )
        (encoder): DebertaV2Encoder(
          (layer): ModuleList(
            (0-11): 12 x DebertaV2Layer(
              (attention): DebertaV2Attention(
                (self): DisentangledSelfAttention(
                  (query_proj): Linear(in_features=768, out_features=768, bias=True)
                  (key_proj): Linear(in_features=768, out_features=768, bias=True)
                  (value_proj): Linear(in_features=768, out_features=768, bias=True)
                  (pos_dropout): StableDropout()
                  (dropout): StableDropout()
                )
                (output): DebertaV2SelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                  (dropout): StableDropout()
                )
              )
              (intermediate): DebertaV2Intermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): DebertaV2Output(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
                (dropout): StableDropout()
              )
            )
          )
          (rel_embeddings): Embedding(512, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
        )
      )
      (dropout): Dropout(p=0.5, inplace=False)
      (post_encoder): Encoder(
        (encoder): ModuleList(
          (0): SelfAttention(
            (SA): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (tanh): Tanh()
      )
      (post_encoder_): Encoder(
        (encoder): ModuleList(
          (0): SelfAttention(
            (SA): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (tanh): Tanh()
      )
      (bert_pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
      (CDW_LSA): LSA(
        (encoder): Encoder(
          (encoder): ModuleList(
            (0): SelfAttention(
              (SA): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (tanh): Tanh()
        )
        (encoder_left): Encoder(
          (encoder): ModuleList(
            (0): SelfAttention(
              (SA): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (tanh): Tanh()
        )
        (encoder_right): Encoder(
          (encoder): ModuleList(
            (0): SelfAttention(
              (SA): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (tanh): Tanh()
        )
        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)
        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)
      )
      (post_linear): Linear(in_features=1536, out_features=768, bias=True)
      (dense): Linear(in_features=768, out_features=3, bias=True)
    )
  )
  (bert): DebertaV2Model(
    (embeddings): DebertaV2Embeddings(
      (word_embeddings): Embedding(128100, 768, padding_idx=0)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
      (dropout): StableDropout()
    )
    (encoder): DebertaV2Encoder(
      (layer): ModuleList(
        (0-11): 12 x DebertaV2Layer(
          (attention): DebertaV2Attention(
            (self): DisentangledSelfAttention(
              (query_proj): Linear(in_features=768, out_features=768, bias=True)
              (key_proj): Linear(in_features=768, out_features=768, bias=True)
              (value_proj): Linear(in_features=768, out_features=768, bias=True)
              (pos_dropout): StableDropout()
              (dropout): StableDropout()
            )
            (output): DebertaV2SelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
              (dropout): StableDropout()
            )
          )
          (intermediate): DebertaV2Intermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): DebertaV2Output(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
            (dropout): StableDropout()
          )
        )
      )
      (rel_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)
    )
  )
  (dense): Linear(in_features=3, out_features=3, bias=True)
)
2023-04-14 16:36:22,452 INFO: ABSADatasetsVersion:None	-->	Calling Count:0
2023-04-14 16:36:22,452 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x00000153D948ADA0>	-->	Calling Count:0
2023-04-14 16:36:22,452 INFO: PyABSAVersion:2.2.1	-->	Calling Count:1
2023-04-14 16:36:22,452 INFO: SRD:3	-->	Calling Count:928
2023-04-14 16:36:22,452 INFO: TorchVersion:2.0.0+cpu+cudaNone	-->	Calling Count:1
2023-04-14 16:36:22,452 INFO: TransformersVersion:4.27.4	-->	Calling Count:1
2023-04-14 16:36:22,452 INFO: auto_device:True	-->	Calling Count:3
2023-04-14 16:36:22,452 INFO: batch_size:16	-->	Calling Count:2
2023-04-14 16:36:22,452 INFO: cache_dataset:True	-->	Calling Count:1
2023-04-14 16:36:22,452 INFO: checkpoint_save_mode:1	-->	Calling Count:4
2023-04-14 16:36:22,452 INFO: cross_validate_fold:-1	-->	Calling Count:1
2023-04-14 16:36:22,452 INFO: dataset_file:{'train': ['.\\integrated_datasets\\apc_datasets\\100.CustomDataset\\custom.train.txt'], 'test': ['.\\integrated_datasets\\apc_datasets\\100.CustomDataset\\custom.test.txt'], 'valid': []}	-->	Calling Count:15
2023-04-14 16:36:22,452 INFO: dataset_name:custom_dataset	-->	Calling Count:3
2023-04-14 16:36:22,452 INFO: dca_layer:3	-->	Calling Count:0
2023-04-14 16:36:22,452 INFO: dca_p:1	-->	Calling Count:0
2023-04-14 16:36:22,452 INFO: deep_ensemble:False	-->	Calling Count:0
2023-04-14 16:36:22,452 INFO: device:cpu	-->	Calling Count:3
2023-04-14 16:36:22,453 INFO: device_name:Unknown	-->	Calling Count:1
2023-04-14 16:36:22,453 INFO: dlcf_a:2	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: dropout:0.5	-->	Calling Count:1
2023-04-14 16:36:22,453 INFO: dynamic_truncate:True	-->	Calling Count:928
2023-04-14 16:36:22,453 INFO: embed_dim:768	-->	Calling Count:7
2023-04-14 16:36:22,453 INFO: eta:1	-->	Calling Count:2
2023-04-14 16:36:22,453 INFO: eta_lr:0.1	-->	Calling Count:1
2023-04-14 16:36:22,453 INFO: evaluate_begin:0	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: from_checkpoint:english	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: hidden_dim:768	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}	-->	Calling Count:2
2023-04-14 16:36:22,453 INFO: inference_model:None	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: initializer:xavier_uniform_	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: inputs_cols:['lcf_cdm_vec', 'lcf_cdw_vec', 'left_lcf_cdm_vec', 'left_lcf_cdw_vec', 'right_lcf_cdm_vec', 'right_lcf_cdw_vec', 'spc_mask_vec', 'text_indices']	-->	Calling Count:6965
2023-04-14 16:36:22,453 INFO: l2reg:1e-06	-->	Calling Count:2
2023-04-14 16:36:22,453 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: lcf:cdw	-->	Calling Count:3
2023-04-14 16:36:22,453 INFO: learning_rate:2e-05	-->	Calling Count:1
2023-04-14 16:36:22,453 INFO: load_aug:False	-->	Calling Count:1
2023-04-14 16:36:22,453 INFO: log_step:5	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: logger:<Logger fast_lsa_t_v2 (INFO)>	-->	Calling Count:14
2023-04-14 16:36:22,453 INFO: lsa:False	-->	Calling Count:0
2023-04-14 16:36:22,453 INFO: max_seq_len:80	-->	Calling Count:5568
2023-04-14 16:36:22,453 INFO: model:<class 'pyabsa.tasks.AspectPolarityClassification.models.__lcf__.fast_lsa_t_v2.FAST_LSA_T_V2'>	-->	Calling Count:6
2023-04-14 16:36:22,454 INFO: model_name:fast_lsa_t_v2	-->	Calling Count:930
2023-04-14 16:36:22,454 INFO: model_path_to_save:checkpoints	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: num_epoch:1	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: optimizer:adamw	-->	Calling Count:1
2023-04-14 16:36:22,454 INFO: output_dim:3	-->	Calling Count:3
2023-04-14 16:36:22,454 INFO: overwrite_cache:False	-->	Calling Count:1
2023-04-14 16:36:22,454 INFO: path_to_save:None	-->	Calling Count:1
2023-04-14 16:36:22,454 INFO: patience:99999	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: pretrained_bert:yangheng/deberta-v3-base-absa-v1.1	-->	Calling Count:5
2023-04-14 16:36:22,454 INFO: save_mode:1	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: seed:52	-->	Calling Count:7
2023-04-14 16:36:22,454 INFO: sigma:0.3	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: similarity_threshold:1	-->	Calling Count:2
2023-04-14 16:36:22,454 INFO: spacy_model:en_core_web_sm	-->	Calling Count:3
2023-04-14 16:36:22,454 INFO: srd_alignment:True	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: task_code:APC	-->	Calling Count:1
2023-04-14 16:36:22,454 INFO: task_name:Aspect-based Sentiment Classification	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='yangheng/deberta-v3-base-absa-v1.1', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: use_amp:False	-->	Calling Count:1
2023-04-14 16:36:22,454 INFO: use_bert_spc:True	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: use_syntax_based_SRD:False	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: warmup_step:-1	-->	Calling Count:0
2023-04-14 16:36:22,454 INFO: window:lr	-->	Calling Count:0
[2023-04-14 16:36:22] (2.2.1) ********** Available APC model checkpoints for Version:2.2.1 (this version) **********
[2023-04-14 16:36:22] (2.2.1) Downloading checkpoint:english 
[2023-04-14 16:36:22] (2.2.1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets
Downloading checkpoint: 602MB [04:49,  2.08MB/s]
Find zipped checkpoint: ./checkpoints\APC_ENGLISH_CHECKPOINT\fast_lsa_t_v2_English_acc_82.21_f1_81.81.zip, unzipping
Done.
[2023-04-14 16:41:17] (2.2.1) If the auto-downloading failed, please download it via browser: https://huggingface.co/spaces/yangheng/PyABSA/resolve/main/checkpoints/English/APC/fast_lsa_t_v2_English_acc_82.21_f1_81.81.zip 
2023-04-14 16:41:17,907 INFO: Checkpoint downloaded at: checkpoints\APC_ENGLISH_CHECKPOINT\fast_lsa_t_v2_English_acc_82.21_f1_81.81
C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\lib\site-packages\pyabsa\framework\instructor_class\instructor_template.py:434: ResourceWarning: unclosed file <_io.BufferedReader name='checkpoints\\APC_ENGLISH_CHECKPOINT\\fast_lsa_t_v2_English_acc_82.21_f1_81.81\\fast_lsa_t_v2.config'>
  config = pickle.load(open(config_path[0], "rb"))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
2023-04-14 16:41:18,915 INFO: Resume trainer from Checkpoint: checkpoints\APC_ENGLISH_CHECKPOINT\fast_lsa_t_v2_English_acc_82.21_f1_81.81!
2023-04-14 16:41:18,919 INFO: ***** Running training for Aspect-based Sentiment Classification *****
2023-04-14 16:41:18,919 INFO: Training set examples = 377
2023-04-14 16:41:18,919 INFO: Test set examples = 87
2023-04-14 16:41:18,919 INFO: Total params = 197414417, Trainable params = 197414417, Non-trainable params = 0
2023-04-14 16:41:18,919 INFO: Batch size = 16
2023-04-14 16:41:18,919 INFO: Num steps = 1
Epoch:0 | Loss:0:   0%|          | 0/24 [00:00<?, ?it/s][2023-04-14 16:41:23] (2.2.1) reset eta1 to: 0.34111571311950684
[2023-04-14 16:41:23] (2.2.1) reset eta2 to: 0.029173076152801514
Epoch:  0 | Smooth Loss: 0.3421: 100%|██████████| 24/24 [05:46<00:00, 14.43s/it, Dev Acc:91.95(max:91.95) Dev F1:67.42(max:67.42)]
C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\lib\site-packages\metric_visualizer\utils.py:25: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.
  self.skewness = stats.skew(self.data, keepdims=True)
2023-04-14 16:47:05,459 INFO: 
----------------------------------------------------------------------- Raw Metric Records -----------------------------------------------------------------------
╒════════════════════════════╤═════════════════════════════════════════════════════════════════╤══════════╤═══════════╤══════════╤═══════╤═══════╤═══════╤═══════╕
│ Metric                     │ Trial                                                           │ Values   │  Average  │  Median  │  Std  │  IQR  │  Min  │  Max  │
╞════════════════════════════╪═════════════════════════════════════════════════════════════════╪══════════╪═══════════╪══════════╪═══════╪═══════╪═══════╪═══════╡
│ Max-Test-Acc w/o Valid Set │ fast_lsa_t_v2-custom_dataset-yangheng/deberta-v3-base-absa-v1.1 │ [91.95]  │   91.95   │  91.95   │   0   │   0   │ 91.95 │ 91.95 │
├────────────────────────────┼─────────────────────────────────────────────────────────────────┼──────────┼───────────┼──────────┼───────┼───────┼───────┼───────┤
│ Max-Test-F1 w/o Valid Set  │ fast_lsa_t_v2-custom_dataset-yangheng/deberta-v3-base-absa-v1.1 │ [67.42]  │   67.42   │  67.42   │   0   │   0   │ 67.42 │ 67.42 │
╘════════════════════════════╧═════════════════════════════════════════════════════════════════╧══════════╧═══════════╧══════════╧═══════╧═══════╧═══════╧═══════╛
-------------------------------------------------------- https://github.com/yangheng95/metric_visualizer --------------------------------------------------------

C:\Users\Marija\Desktop\ABSADatasets-2.0\venv\lib\site-packages\pyabsa\framework\trainer_class\trainer_template.py:251: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\Users\\Marija\\Desktop\\ABSADatasets-2.0\\logs\\fast_lsa_t_v2_20230414 163603\\trainer.log' mode='a' encoding='utf8'>
  self.config.logger.removeHandler(self.config.logger.handlers[0])
ResourceWarning: Enable tracemalloc to get the object allocation traceback

Process finished with exit code 0